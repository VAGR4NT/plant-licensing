# -*- coding: utf-8 -*-
"""ViT distillation learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MLDp648b4Ld-gDs8vXSJQCxqHjLZfXp6
"""

from datasets import load_dataset, DatasetDict
from transformers import ViTImageProcessor, ViTForImageClassification
from transformers import TrainingArguments
from transformers import Trainer

import torch
import torch.nn as nn
import torch.nn.functional as F

from torchvision import transforms
from PIL import Image

import numpy as np
import evaluate

"""## Load Data"""

from datasets import load_dataset, DatasetDict

# Use the recommended repository that doesn't rely on a deprecated loading script
dataset_id = "zh-plus/tiny-imagenet"

# 1. Load the original 'train' split (100,000 images)
original_train_set = load_dataset(dataset_id, split="train")

# 2. Split the original train set into a new 80% train and 20% temporary split
# The temporary split (20%) will be 20,000 images.
train_validation_test_splits = original_train_set.train_test_split(
    test_size=0.2, # 20% for validation + test
    seed=42 # Set a seed for reproducibility
)

# Rename the splits for clarity
new_train_set = train_validation_test_splits["train"] # 80,000 images (80%)
temp_split = train_validation_test_splits["test"]     # 20,000 images (20%)

# 3. Split the temporary 20% (20,000 images) equally into new validation and test sets
# This results in 10,000 images each, making them 10% of the original 100k.
validation_test_splits = temp_split.train_test_split(
    test_size=0.5, # 50% of the 20% for test (i.e., 10% overall)
    seed=42
)

# Final resulting splits:
new_validation_set = validation_test_splits["train"] # 10,000 images (10%)
new_test_set = validation_test_splits["test"]       # 10,000 images (10%)

# 4. Create a final DatasetDict for clear management
tiny_imagenet_splits = DatasetDict({
    "train": new_train_set,
    "validation": new_validation_set,
    "test": new_test_set
})

print("--- New Dataset Split Achieved (80/10/10) ---")
print(f"New Train Size:      {len(tiny_imagenet_splits['train'])}")     # ~80,000
print(f"New Validation Size: {len(tiny_imagenet_splits['validation'])}") # ~10,000
print(f"New Test Size:       {len(tiny_imagenet_splits['test'])}")       # ~10,000

# Note: The original 'valid' split from the dataset is discarded.

"""## Data Preprocessing"""

# Define the target size
TARGET_SIZE = 224

# ImageNet means and standard deviations (used for standard transfer learning)
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]

# --- 1. Transforms for TRAINING Data (with Augmentation) ---
# Data augmentation helps the model generalize better by introducing variations.
train_transforms = transforms.Compose([
    # Resize and randomly crop to 224x224. This is a powerful augmentation.
    transforms.RandomResizedCrop(TARGET_SIZE, scale=(0.8, 1.0)),
    # Horizontal flipping is a common and cheap augmentation.
    transforms.RandomHorizontalFlip(),
    # Convert the PIL image to a PyTorch Tensor (scales to [0, 1])
    transforms.ToTensor(),
    # Normalize the tensor channels
    transforms.Normalize(mean, std)
])

# --- 2. Transforms for VALIDATION/TEST Data (Deterministic) ---
# These sets must be treated identically and without random augmentation.
val_test_transforms = transforms.Compose([
    # Resize the smaller edge to 224, then take a 224x224 crop from the center.
    transforms.Resize(256), # Scale up slightly before cropping
    transforms.CenterCrop(TARGET_SIZE), # Take a deterministic center crop
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])

# --- 3. Mapping Functions for Hugging Face Dataset ---

def preprocess_train(examples):
    """Applies training transforms to a batch of images."""
    # The 'image' column contains PIL images. Convert them to RGB first.
    examples["pixel_values"] = [train_transforms(image.convert("RGB")) for image in examples["image"]]
    return examples

def preprocess_validation_or_test(examples):
    """Applies deterministic validation/test transforms to a batch of images."""
    examples["pixel_values"] = [val_test_transforms(image.convert("RGB")) for image in examples["image"]]
    return examples

# The functions are ready to be used with the .map() method on your new dataset splits.
# Example usage (assuming you have a DatasetDict named tiny_imagenet_splits):
# tiny_imagenet_splits["train"] = tiny_imagenet_splits["train"].map(preprocess_train, batched=True, remove_columns=["image"])
# tiny_imagenet_splits["validation"] = tiny_imagenet_splits["validation"].map(preprocess_validation_or_test, batched=True, remove_columns=["image"])
# tiny_imagenet_splits["test"] = tiny_imagenet_splits["test"].map(preprocess_validation_or_test, batched=True, remove_columns=["image"])

print("--- Applying Preprocessing (Saving Results) ---")

# --- Training Split ---
tiny_imagenet_splits["train"] = tiny_imagenet_splits["train"].map(
    preprocess_train,
    batched=True,
    # This removes the original 'image' column
    remove_columns=['image']
)

# --- Validation Split ---
tiny_imagenet_splits["validation"] = tiny_imagenet_splits["validation"].map(
    preprocess_validation_or_test,
    batched=True,
    remove_columns=['image']
)

# --- Test Split ---
tiny_imagenet_splits["test"] = tiny_imagenet_splits["test"].map(
    preprocess_validation_or_test,
    batched=True,
    remove_columns=['image']
)

print("--- Renaming and Setting Format ---")

# The Trainer expects 'labels' and the data collator expects 'pixel_values'.
for split in tiny_imagenet_splits:
    # Rename 'label' to 'labels'
    tiny_imagenet_splits[split] = tiny_imagenet_splits[split].rename_column("label", "labels")

    # Set the format for PyTorch, exposing only the needed columns
    tiny_imagenet_splits[split].set_format(type="torch", columns=["pixel_values", "labels"])

print("Preprocessing complete. Datasets ready for Trainer.")

# Double-check: The features should now only show 'pixel_values' and 'labels'
print("Final Train Features:", tiny_imagenet_splits["train"].features)

"""## CNN (student)"""

# --- Constants for Tiny-ImageNet ---
NUM_CLASSES = 200
# Calculated feature map size after 3 MaxPool2D layers on a 224x224 input: 28x28
FC_INPUT_SIZE = 28 * 28 * 64

class SmallCNN(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(SmallCNN, self).__init__()

        # --- 1. Feature Extractor (224x224 Input) ---

        # Block 1: 224x224x3 -> 112x112x16
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Block 2: 112x112x16 -> 56x56x32
        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Block 3: 56x56x32 -> 28x28x64
        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        # --- 2. Classifier (Fully Connected Layers) ---

        # Input size: 28 * 28 * 64 = 50176
        self.fc1 = nn.Linear(FC_INPUT_SIZE, 512)
        self.fc2 = nn.Linear(512, num_classes)

        # --- 3. Loss Function ---
        # The Trainer expects CrossEntropyLoss for standard classification
        self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, pixel_values, labels=None):
        """
        The forward method accepts 'pixel_values' for the input tensor
        and 'labels' (set to None by default) to align with the Hugging Face Trainer.
        """
        # Rename for internal processing
        x = pixel_values

        # Feature extraction
        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        x = self.pool3(F.relu(self.conv3(x)))

        # Flatten
        x = x.view(-1, FC_INPUT_SIZE)

        # Classification (produces logits)
        x = F.relu(self.fc1(x))
        logits = self.fc2(x)

        # --- Trainer Integration Fix ---
        # If labels are provided (during training), calculate and return loss and logits
        if labels is not None:
            # Calculate the scalar loss
            loss = self.loss_fn(logits, labels)

            # Returning (loss, logits) resolves the "grad can be implicitly created only for scalar outputs" error
            return (loss, logits)

        # If no labels are provided (during evaluation/prediction), return just the logits
        return logits

"""## Training Setup"""

def data_collator_image_classification(examples):
    """
    Collates a list of examples into a single batch dictionary.

    Args:
        examples (List[Dict]): A list of dataset examples, each containing
                                'pixel_values' (torch.Tensor) and 'labels' (int).

    Returns:
        Dict: A dictionary containing the batched pixel values and labels.
    """
    # Stack the list of image tensors into a single batch tensor
    pixel_values = torch.stack([example["pixel_values"] for example in examples])

    # Stack the list of labels into a single tensor, ensuring it's LongTensor for classification
    labels = torch.tensor([example["labels"] for example in examples], dtype=torch.long)

    return {
        "pixel_values": pixel_values,
        "labels": labels
    }

# Load accuracy metric
accuracy_metric = evaluate.load("accuracy")

def compute_metrics(p):
    """
    Computes accuracy given the prediction object from the Trainer.
    """
    # p.predictions are the raw logits (un-normalized outputs of the model)
    # np.argmax selects the index (class ID) with the highest logit
    preds = np.argmax(p.predictions, axis=1)

    # Compare predictions to the true labels (p.label_ids)
    return accuracy_metric.compute(predictions=preds, references=p.label_ids)

# Tiny-ImageNet has 200 classes
NUM_CLASSES = 200

training_args = TrainingArguments(
    output_dir="./tiny_imagenet_cnn_results",   # Directory to save checkpoints and logs
    num_train_epochs=10,                       # Start with a moderate number of epochs
    per_device_train_batch_size=64,            # Adjust based on your GPU memory
    per_device_eval_batch_size=64,             # Batch size for evaluation
    learning_rate=5e-5,                        # A standard starting learning rate
    warmup_steps=500,                          # Steps for the learning rate to ramp up
    weight_decay=0.01,                         # L2 regularization to help prevent overfitting
    logging_dir='./logs',                      # Directory for storing TensorBoard logs
    logging_steps=200,                         # Log progress every 200 steps
    eval_strategy="epoch",               # Evaluate on the validation set after each epoch
    save_strategy="epoch",                     # Save a checkpoint after each epoch
    load_best_model_at_end=True,               # Use the validation set to find and load the best model
    metric_for_best_model="accuracy",          # Monitor accuracy on the validation set
    greater_is_better=True,
    # This is important for custom models to avoid dropping the 'pixel_values' column
    remove_unused_columns=False,
)

"""## Train Model"""

print(tiny_imagenet_splits["train"].features)



# Assuming you have loaded and preprocessed your data into a DatasetDict
# with keys 'train', 'validation', 'test' (as discussed previously)
# Example names: tiny_imagenet_splits['train'], tiny_imagenet_splits['validation']

# 1. Instantiate the SmallCNN model (from the previous step)
# model = SmallCNN(num_classes=NUM_CLASSES)

# 2. Instantiate the Trainer
trainer = Trainer(
    model=model,                                       # The SmallCNN model
    args=training_args,                                # Training hyperparameters
    train_dataset=tiny_imagenet_splits["train"],       # New 80% train split
    eval_dataset=tiny_imagenet_splits["validation"],   # New 10% validation split
    compute_metrics=compute_metrics,                   # Metric function
    data_collator=data_collator_image_classification,  # Our custom collator
)

# 3. Start training!
print("Starting training of SmallCNN...")
train_result = trainer.train()

# 4. Final Evaluation on the held-out TEST set
# This provides the true, unbiased generalization score.
print("\nEvaluating on the independent TEST set...")
test_results = trainer.evaluate(tiny_imagenet_splits["test"])

print(f"Final Test Accuracy: {test_results['eval_accuracy']:.4f}")